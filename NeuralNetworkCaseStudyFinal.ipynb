{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GESTURE RECOGNITION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed Libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Supress all the warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the random seet and keras, tensorflow\n",
    "\n",
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We have changed the right path where the files are stored above.\n",
    "\n",
    "train_doc = np.random.permutation(open('./Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('./Project_data/val.csv').readlines())\n",
    "batch_size =  40 #experiment with the batch size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generator function for input data without augmentation.\n",
    "\n",
    "x = 30 # No. of frames images\n",
    "y = 120 # Width of the image\n",
    "z = 120 # height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    # Let us resize all the images.Let's use PIL.Image.NEAREST (use nearest neighbour) resampling filter. \n",
    "                    resized_image = imresize(image,(y,z)) ##default resample=1 or 'P' which indicates PIL.Image.NEAREST\n",
    "                    resized_image = resized_image/255\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (resized_image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (resized_image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (resized_image[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches        \n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    resized_image = imresize(image,(y,z)) ##default resample=1 or 'P' which indicates PIL.Image.NEAREST\n",
    "                    resized_image = resized_image/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (resized_image[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (resized_image[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (resized_image[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training_Sequences = 663\n",
      "# Validation_Sequences = 100\n",
      "# Epochs =  15\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = './Project_data/train'\n",
    "val_path = './Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# Training_Sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# Validation_Sequences =', num_val_sequences)\n",
    "num_epochs = 15 # choose the number of epochs\n",
    "print ('# Epochs = ', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 : Basic Model to test if our network is working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 40 \n",
    "- Image Height : 120 \n",
    "- Image Width : 120 \n",
    "- Epochs - 15 \n",
    "- Optimizer - Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us import all the needed libraries of Keras.\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "#write your model here\n",
    "# Input all the images sequential by building the layer with dropouts and batchnormalisation\n",
    "\n",
    "model_1 = Sequential()       \n",
    "model_1.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(30, 120, 120, 3),padding='same'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "model_1.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_1.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_1.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_1.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_1.add(Flatten())\n",
    "\n",
    "model_1.add(Dense(1000, activation='relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "\n",
    "model_1.add(Dense(500, activation='relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_1.add(Dense(5, activation='softmax'))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 30, 120, 120, 16)  3472      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 15, 60, 60, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 7, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 3, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              6273000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train and validate the model \n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see the Validate the Losses and put back the checkpoint\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see that the steps_per_epoch and validation steps are used by fit_generator to decide the no. of next()\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "17/17 [==============================] - 94s 6s/step - loss: 7.4637 - categorical_accuracy: 0.2926 - val_loss: 4.6685 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0709_59_17.824524/model-00001-7.52395-0.28808-4.66847-0.50000.h5\n",
      "Epoch 2/15\n",
      "17/17 [==============================] - 37s 2s/step - loss: 4.3265 - categorical_accuracy: 0.4655 - val_loss: 6.0890 - val_categorical_accuracy: 0.3667\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0709_59_17.824524/model-00002-4.32650-0.46547-6.08896-0.36667.h5\n",
      "Epoch 3/15\n",
      "17/17 [==============================] - 43s 3s/step - loss: 2.7872 - categorical_accuracy: 0.4854 - val_loss: 2.6283 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0709_59_17.824524/model-00003-2.81722-0.48501-2.62826-0.45000.h5\n",
      "Epoch 4/15\n",
      "17/17 [==============================] - 34s 2s/step - loss: 2.2697 - categorical_accuracy: 0.5232 - val_loss: 2.0574 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0709_59_17.824524/model-00004-2.26966-0.52322-2.05737-0.55000.h5\n",
      "Epoch 5/15\n",
      "17/17 [==============================] - 37s 2s/step - loss: 1.6859 - categorical_accuracy: 0.5762 - val_loss: 2.0350 - val_categorical_accuracy: 0.3833\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0709_59_17.824524/model-00005-1.69851-0.57508-2.03499-0.38333.h5\n",
      "Epoch 6/15\n",
      "17/17 [==============================] - 31s 2s/step - loss: 1.4817 - categorical_accuracy: 0.6021 - val_loss: 1.1587 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0709_59_17.824524/model-00006-1.48170-0.60208-1.15868-0.60000.h5\n",
      "Epoch 7/15\n",
      "17/17 [==============================] - 32s 2s/step - loss: 1.2458 - categorical_accuracy: 0.6298 - val_loss: 2.0828 - val_categorical_accuracy: 0.4833\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0709_59_17.824524/model-00007-1.24578-0.62976-2.08285-0.48333.h5\n",
      "Epoch 8/15\n",
      "17/17 [==============================] - 33s 2s/step - loss: 1.0504 - categorical_accuracy: 0.6782 - val_loss: 1.5352 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0709_59_17.824524/model-00008-1.05035-0.67820-1.53517-0.61667.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/15\n",
      "17/17 [==============================] - 32s 2s/step - loss: 1.1250 - categorical_accuracy: 0.6574 - val_loss: 0.6826 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0709_59_17.824524/model-00009-1.12501-0.65744-0.68258-0.71667.h5\n",
      "Epoch 10/15\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.6497 - categorical_accuracy: 0.7439 - val_loss: 1.3210 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0709_59_17.824524/model-00010-0.64966-0.74394-1.32099-0.60000.h5\n",
      "Epoch 11/15\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.6659 - categorical_accuracy: 0.7785 - val_loss: 0.4395 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0709_59_17.824524/model-00011-0.66595-0.77855-0.43955-0.80000.h5\n",
      "Epoch 12/15\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.5192 - categorical_accuracy: 0.8270 - val_loss: 0.6141 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0709_59_17.824524/model-00012-0.51916-0.82699-0.61405-0.75000.h5\n",
      "Epoch 13/15\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.3739 - categorical_accuracy: 0.8685 - val_loss: 0.6635 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0709_59_17.824524/model-00013-0.37390-0.86851-0.66352-0.75000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 14/15\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.3876 - categorical_accuracy: 0.8339 - val_loss: 0.7997 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0709_59_17.824524/model-00014-0.38761-0.83391-0.79965-0.73333.h5\n",
      "Epoch 15/15\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.2895 - categorical_accuracy: 0.8858 - val_loss: 0.7063 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0709_59_17.824524/model-00015-0.28948-0.88581-0.70633-0.75000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a89509908>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us fit the model\n",
    "\n",
    "model_1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "- ***Best Training Accuracy - 86.85 %***\n",
    "- ***Best Validation Accuracy - 75.00 %***\n",
    "\n",
    "\n",
    "This is the most initial model, it is difficult to conclude that this will be our final model without hyper parameter tuning, so in upcoming models , we will experiment with different hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 : Hyperparameter Tuned : Activation Function and Optimiser\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Changing activation function from 'Relu' to 'elu' since we are not working on non negative data, ELU can also be a good activation function. \n",
    "\n",
    "- Changing optimiser to SGD to check how the data is divided into smaller batches and uses a stochastic gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 40 \n",
    "- Image Height : 120 \n",
    "- Image Width : 120 \n",
    "- Epochs - 15 \n",
    "- Optimizer - SGD\n",
    "- Activation Function : ELU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_6 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 30, 120, 120, 16)  3472      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 15, 60, 60, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 7, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 3, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              6273000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Input all the images sequencial by building the layer with dropouts and batchnormalisation\n",
    "\n",
    "model_2 = Sequential()       \n",
    "model_2.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(30, 120, 120, 3),padding='same'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(Activation('elu'))\n",
    "\n",
    "model_2.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "model_2.add(Activation('elu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_2.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_2.add(Activation('elu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_2.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_2.add(Activation('elu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_2.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_2.add(Activation('elu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_2.add(Flatten())\n",
    "\n",
    "model_2.add(Dense(1000, activation='elu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "\n",
    "model_2.add(Dense(500, activation='elu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_2.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser =optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True) #write your optimizer\n",
    "model_2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train and validate the model \n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "17/17 [==============================] - 82s 5s/step - loss: 3.1734 - categorical_accuracy: 0.2798 - val_loss: 1.4583 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0709_59_17.824524/model-00001-3.20075-0.27602-1.45831-0.42000.h5\n",
      "Epoch 2/15\n",
      "17/17 [==============================] - 39s 2s/step - loss: 2.1520 - categorical_accuracy: 0.4476 - val_loss: 1.0073 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0709_59_17.824524/model-00002-2.15202-0.44757-1.00729-0.60000.h5\n",
      "Epoch 3/15\n",
      "17/17 [==============================] - 41s 2s/step - loss: 1.7799 - categorical_accuracy: 0.4997 - val_loss: 1.1113 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0709_59_17.824524/model-00003-1.80156-0.49591-1.11126-0.61667.h5\n",
      "Epoch 4/15\n",
      "17/17 [==============================] - 36s 2s/step - loss: 1.5438 - categorical_accuracy: 0.5480 - val_loss: 0.9282 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0709_59_17.824524/model-00004-1.54384-0.54799-0.92820-0.61667.h5\n",
      "Epoch 5/15\n",
      "17/17 [==============================] - 34s 2s/step - loss: 1.2879 - categorical_accuracy: 0.5737 - val_loss: 1.2524 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0709_59_17.824524/model-00005-1.29626-0.57188-1.25243-0.58333.h5\n",
      "Epoch 6/15\n",
      "17/17 [==============================] - 33s 2s/step - loss: 1.2142 - categorical_accuracy: 0.6228 - val_loss: 1.3836 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0709_59_17.824524/model-00006-1.21425-0.62284-1.38362-0.60000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/15\n",
      "17/17 [==============================] - 33s 2s/step - loss: 1.2733 - categorical_accuracy: 0.6159 - val_loss: 1.1352 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0709_59_17.824524/model-00007-1.27329-0.61592-1.13523-0.58333.h5\n",
      "Epoch 8/15\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.9314 - categorical_accuracy: 0.6817 - val_loss: 0.8744 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0709_59_17.824524/model-00008-0.93137-0.68166-0.87441-0.71667.h5\n",
      "Epoch 9/15\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.8749 - categorical_accuracy: 0.7163 - val_loss: 0.8162 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0709_59_17.824524/model-00009-0.87494-0.71626-0.81616-0.73333.h5\n",
      "Epoch 10/15\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.7818 - categorical_accuracy: 0.7128 - val_loss: 0.8298 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0709_59_17.824524/model-00010-0.78176-0.71280-0.82976-0.75000.h5\n",
      "Epoch 11/15\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.8396 - categorical_accuracy: 0.6886 - val_loss: 0.7837 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0709_59_17.824524/model-00011-0.83960-0.68858-0.78366-0.71667.h5\n",
      "Epoch 12/15\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.7493 - categorical_accuracy: 0.7370 - val_loss: 0.6392 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0709_59_17.824524/model-00012-0.74932-0.73702-0.63920-0.75000.h5\n",
      "Epoch 13/15\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.7460 - categorical_accuracy: 0.7336 - val_loss: 0.9259 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0709_59_17.824524/model-00013-0.74605-0.73356-0.92592-0.78333.h5\n",
      "Epoch 14/15\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.6817 - categorical_accuracy: 0.7543 - val_loss: 0.7887 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0709_59_17.824524/model-00014-0.68165-0.75433-0.78869-0.71667.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 15/15\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.6163 - categorical_accuracy: 0.7785 - val_loss: 0.5665 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0709_59_17.824524/model-00015-0.61632-0.77855-0.56652-0.80000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a2823a898>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us fit the model\n",
    "\n",
    "model_2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "- ***Best Training Accuracy - 75.4 %***\n",
    "- ***Best Validation Accuracy - 71.67 %***\n",
    "\n",
    "\n",
    "From our model we can see that our validation accuracy is considerably higher than training accuracy in most of the cases. \n",
    "\n",
    "- This could be due to high dropouts since we are using 0.5 \n",
    "- Very basic model with inadequate amount of data\n",
    "- Indicates high bias in the neural network \n",
    "\n",
    "\n",
    "For all the above reasons, it is better to tune more hyperparamters with the initial model (Model - 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model -  3 Hyperparameter Tuned : Epochs (Model 1 - Initial Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 40 \n",
    "- Image Height : 120 \n",
    "- Image Width : 120 \n",
    "- Epochs - 25 \n",
    "- Optimizer - Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_11 (Conv3D)           (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 30, 120, 120, 16)  3472      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_13 (Conv3D)           (None, 15, 60, 60, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 7, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 3, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1000)              6273000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let us experiment different x,y,z value in the CNN network and find tune all the image size & Hyperparameters later\n",
    "\n",
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "# Input all the images sequencial by building the layer with dropouts and batchnormalisation\n",
    "\n",
    "model_3 = Sequential()       \n",
    "model_3.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(x,y,z,3),padding='same'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Activation('relu'))\n",
    "\n",
    "model_3.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_3.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_3.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_3.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_3.add(Flatten())\n",
    "\n",
    "model_3.add(Dense(1000, activation='relu'))\n",
    "model_3.add(Dropout(0.5))\n",
    "\n",
    "model_3.add(Dense(500, activation='relu'))\n",
    "model_3.add(Dropout(0.55))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_3.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train and validate the model \n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "17/17 [==============================] - 84s 5s/step - loss: 7.9792 - categorical_accuracy: 0.3113 - val_loss: 12.3609 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0709_59_17.824524/model-00001-8.00286-0.30618-12.36093-0.19000.h5\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - 41s 2s/step - loss: 7.8758 - categorical_accuracy: 0.3836 - val_loss: 9.9560 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0709_59_17.824524/model-00002-7.87575-0.38363-9.95596-0.33333.h5\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - 40s 2s/step - loss: 6.7155 - categorical_accuracy: 0.4469 - val_loss: 8.0621 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0709_59_17.824524/model-00003-6.73049-0.44687-8.06205-0.35000.h5\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - 37s 2s/step - loss: 6.3070 - categorical_accuracy: 0.4241 - val_loss: 4.1407 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0709_59_17.824524/model-00004-6.30702-0.42415-4.14065-0.58333.h5\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 5.3393 - categorical_accuracy: 0.4256 - val_loss: 3.9623 - val_categorical_accuracy: 0.4667\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0709_59_17.824524/model-00005-5.33075-0.42812-3.96228-0.46667.h5\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 4.1827 - categorical_accuracy: 0.4983 - val_loss: 3.0665 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0709_59_17.824524/model-00006-4.18274-0.49827-3.06646-0.55000.h5\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 3.8640 - categorical_accuracy: 0.5052 - val_loss: 3.0529 - val_categorical_accuracy: 0.5167\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0709_59_17.824524/model-00007-3.86401-0.50519-3.05288-0.51667.h5\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 2.7762 - categorical_accuracy: 0.5398 - val_loss: 3.3759 - val_categorical_accuracy: 0.4667\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0709_59_17.824524/model-00008-2.77618-0.53979-3.37590-0.46667.h5\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 2.8165 - categorical_accuracy: 0.5640 - val_loss: 2.6221 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0709_59_17.824524/model-00009-2.81648-0.56401-2.62209-0.50000.h5\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - 31s 2s/step - loss: 1.6627 - categorical_accuracy: 0.6125 - val_loss: 1.8500 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0709_59_17.824524/model-00010-1.66267-0.61246-1.85002-0.55000.h5\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 1.3074 - categorical_accuracy: 0.6401 - val_loss: 1.2234 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0709_59_17.824524/model-00011-1.30742-0.64014-1.22343-0.61667.h5\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 1.2996 - categorical_accuracy: 0.6367 - val_loss: 1.5566 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0709_59_17.824524/model-00012-1.29961-0.63668-1.55661-0.68333.h5\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.9880 - categorical_accuracy: 0.7197 - val_loss: 2.0929 - val_categorical_accuracy: 0.4833\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0709_59_17.824524/model-00013-0.98799-0.71972-2.09293-0.48333.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 1.0674 - categorical_accuracy: 0.6747 - val_loss: 0.7707 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0709_59_17.824524/model-00014-1.06744-0.67474-0.77066-0.76667.h5\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.6428 - categorical_accuracy: 0.7958 - val_loss: 0.6287 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0709_59_17.824524/model-00015-0.64276-0.79585-0.62869-0.75000.h5\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.6244 - categorical_accuracy: 0.7855 - val_loss: 0.8380 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0709_59_17.824524/model-00016-0.62438-0.78547-0.83803-0.60000.h5\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.4447 - categorical_accuracy: 0.8270 - val_loss: 0.7018 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0709_59_17.824524/model-00017-0.44471-0.82699-0.70184-0.80000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - 30s 2s/step - loss: 0.4809 - categorical_accuracy: 0.8408 - val_loss: 0.5560 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0709_59_17.824524/model-00018-0.48093-0.84083-0.55599-0.75000.h5\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.3729 - categorical_accuracy: 0.8754 - val_loss: 0.6125 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0709_59_17.824524/model-00019-0.37290-0.87543-0.61253-0.68333.h5\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.4561 - categorical_accuracy: 0.8062 - val_loss: 0.4687 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0709_59_17.824524/model-00020-0.45609-0.80623-0.46874-0.78333.h5\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.3217 - categorical_accuracy: 0.8893 - val_loss: 0.6267 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-02-0709_59_17.824524/model-00021-0.32169-0.88927-0.62672-0.71667.h5\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.3581 - categorical_accuracy: 0.8720 - val_loss: 0.5242 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-02-0709_59_17.824524/model-00022-0.35811-0.87197-0.52418-0.75000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.3149 - categorical_accuracy: 0.8858 - val_loss: 0.5083 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-02-0709_59_17.824524/model-00023-0.31493-0.88581-0.50833-0.78333.h5\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.2718 - categorical_accuracy: 0.9135 - val_loss: 0.5006 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-02-0709_59_17.824524/model-00024-0.27177-0.91349-0.50060-0.75000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.3542 - categorical_accuracy: 0.8858 - val_loss: 0.4166 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-02-0709_59_17.824524/model-00025-0.35424-0.88581-0.41662-0.78333.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a730c6160>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let us fit the model\n",
    "\n",
    "model_3.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "- ***Best Training Accuracy - 82.7 %***\n",
    "- ***Best Validation Accuracy - 80.00 %***\n",
    "\n",
    "We can clearly see that increasing epoch have increased accuracy.\n",
    "\n",
    "The above are best values we got in model-3. Going with epoch-25's values as the difference between training and validation accuracy is <5%.\n",
    "\n",
    "The computation time increases with the number of epochs, however the accuracy also increases and gradually the model runs better. \n",
    "\n",
    "Currently, we have obtained our best h5 model file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 is considered as the Base Model and the hyper parameters are trained on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - 4 : Hyperparameter Tuned : Increase image height and width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 40 \n",
    "- Image Height : 160 \n",
    "- Image Width : 160 \n",
    "- Epochs - 25\n",
    "- Optimizer - Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_16 (Conv3D)           (None, 30, 160, 160, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 30, 160, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 30, 160, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_17 (Conv3D)           (None, 30, 160, 160, 16)  3472      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 30, 160, 160, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 30, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_13 (MaxPooling (None, 15, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 15, 80, 80, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 15, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 15, 80, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_14 (MaxPooling (None, 7, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_19 (Conv3D)           (None, 7, 40, 40, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 7, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 7, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_20 (Conv3D)           (None, 3, 20, 20, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 3, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 3, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1000)              12801000  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 13,395,365\n",
      "Trainable params: 13,394,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let us experiment different x,y,z value in the CNN network and find tune all the image size & Hyperparameters later\n",
    "\n",
    "x = 30 # number of frames\n",
    "y = 160 # image width\n",
    "z = 160 # image height\n",
    "\n",
    "# Input all the images sequencial by building the layer with dropouts and batchnormalisation\n",
    "\n",
    "model_4 = Sequential()       \n",
    "model_4.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(x,y,z,3),padding='same'))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Activation('relu'))\n",
    "\n",
    "model_4.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_4.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_4.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_4.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_4.add(Flatten())\n",
    "\n",
    "model_4.add(Dense(1000, activation='relu'))\n",
    "model_4.add(Dropout(0.5))\n",
    "\n",
    "model_4.add(Dense(500, activation='relu'))\n",
    "model_4.add(Dropout(0.5))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_4.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_4.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train and validate the model \n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[40,16,30,160,160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: max_pooling3d_13/MaxPool3D = MaxPool3D[T=DT_FLOAT, _class=[\"loc:@training_3/Adam/gradients/batch_normalization_17/cond/Merge_grad/cond_grad\"], data_format=\"NDHWC\", ksize=[1, 2, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_17/cond/Merge)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics_3/categorical_accuracy/Mean/_2133 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3226_metrics_3/categorical_accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9698f67041ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m model_4.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n\u001b[1;32m      6\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[40,16,30,160,160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: max_pooling3d_13/MaxPool3D = MaxPool3D[T=DT_FLOAT, _class=[\"loc:@training_3/Adam/gradients/batch_normalization_17/cond/Merge_grad/cond_grad\"], data_format=\"NDHWC\", ksize=[1, 2, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_17/cond/Merge)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics_3/categorical_accuracy/Mean/_2133 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3226_metrics_3/categorical_accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "##Let us fit the model\n",
    "\n",
    "##Commenting as it will throw OOM error\n",
    "##model_4.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "#                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "#                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "***OOM Error - ResourceExhaustedError: OOM when allocating tensor with shape[40,16,30,160,160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc***\n",
    "\n",
    "- ***Best Training Accuracy - None %***\n",
    "- ***Best Validation Accuracy - None %***\n",
    "\n",
    "By increasing the memory the tensor size increases and the GPU outputs results in OOM exception, the resources are not enough to run the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model- 5 : Hyperparameter Tuned  : Image Dimension \n",
    "\n",
    "### Since 160 x 160 threw OOM exception, reduced it to 140 x 140 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 40 \n",
    "- Image Height : 140 \n",
    "- Image Width : 140 \n",
    "- Epochs - 25\n",
    "- Optimizer - Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_26 (Conv3D)           (None, 30, 140, 140, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 30, 140, 140, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 30, 140, 140, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_27 (Conv3D)           (None, 30, 140, 140, 16)  3472      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 30, 140, 140, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 30, 140, 140, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_21 (MaxPooling (None, 15, 70, 70, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_28 (Conv3D)           (None, 15, 70, 70, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 15, 70, 70, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 15, 70, 70, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_22 (MaxPooling (None, 7, 35, 35, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_29 (Conv3D)           (None, 7, 35, 35, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 7, 35, 35, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 7, 35, 35, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_23 (MaxPooling (None, 3, 17, 17, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_30 (Conv3D)           (None, 3, 17, 17, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 3, 17, 17, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 3, 17, 17, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 1, 8, 8, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1000)              8193000   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 8,787,365\n",
      "Trainable params: 8,786,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let us experiment different x,y,z value in the CNN network and find tune all the image size & Hyperparameters later\n",
    "\n",
    "x = 30 # number of frames\n",
    "y = 140 # image width\n",
    "z = 140 # image height\n",
    "\n",
    "# Input all the images sequencial by building the layer with dropouts and batchnormalisation\n",
    "\n",
    "model_5 = Sequential()       \n",
    "model_5.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(x,y,z,3),padding='same'))\n",
    "model_5.add(BatchNormalization())\n",
    "model_5.add(Activation('relu'))\n",
    "\n",
    "model_5.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(BatchNormalization())\n",
    "model_5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_5.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(BatchNormalization())\n",
    "model_5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_5.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(BatchNormalization())\n",
    "model_5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_5.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(BatchNormalization())\n",
    "model_5.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_5.add(Flatten())\n",
    "\n",
    "model_5.add(Dense(1000, activation='relu'))\n",
    "model_5.add(Dropout(0.5))\n",
    "\n",
    "model_5.add(Dense(500, activation='relu'))\n",
    "model_5.add(Dropout(0.5))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_5.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_5.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train and validate the model \n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[40,16,30,140,140] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_4/Adam/gradients/max_pooling3d_21/MaxPool3D_grad/MaxPool3DGrad = MaxPool3DGrad[T=DT_FLOAT, TInput=DT_FLOAT, _class=[\"loc:@training_4/Adam/gradients/batch_normalization_27/cond/Merge_grad/cond_grad\"], data_format=\"NDHWC\", ksize=[1, 2, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_27/cond/Merge, max_pooling3d_21/MaxPool3D, training_4/Adam/gradients/conv3d_28/convolution_grad/Conv3DBackpropInputV2)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1b43acc0c7d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m model_5.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n\u001b[1;32m      6\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[40,16,30,140,140] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_4/Adam/gradients/max_pooling3d_21/MaxPool3D_grad/MaxPool3DGrad = MaxPool3DGrad[T=DT_FLOAT, TInput=DT_FLOAT, _class=[\"loc:@training_4/Adam/gradients/batch_normalization_27/cond/Merge_grad/cond_grad\"], data_format=\"NDHWC\", ksize=[1, 2, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_27/cond/Merge, max_pooling3d_21/MaxPool3D, training_4/Adam/gradients/conv3d_28/convolution_grad/Conv3DBackpropInputV2)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "## Let us fit the model\n",
    "\n",
    "##Commenting as it will throw OOM error\n",
    "\n",
    "#model_5.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "#                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "#                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "***OOM Error - ResourceExhaustedError: OOM when allocating tensor with shape[40,16,30,160,160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc***\n",
    "\n",
    "- ***Best Training Accuracy - None %***\n",
    "- ***Best Validation Accuracy - None %***\n",
    "\n",
    "By increasing the memory the tensor size increases and the GPU outputs results in OOM exception, the resources are not enough to run the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - 6 : Hyperparameter Tuned : Batch Size ( Increased to 50 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 50\n",
    "- Image Height : 120\n",
    "- Image Width : 120\n",
    "- Epochs - 25\n",
    "- Optimizer - Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_31 (Conv3D)           (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_32 (Conv3D)           (None, 30, 120, 120, 16)  3472      \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_25 (MaxPooling (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_33 (Conv3D)           (None, 15, 60, 60, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_26 (MaxPooling (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_34 (Conv3D)           (None, 7, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_27 (MaxPooling (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_35 (Conv3D)           (None, 3, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_28 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1000)              6273000   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let us experiment different x,y,z value in the CNN network and find tune all the image size & Hyperparameters later\n",
    "\n",
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "# Input all the images sequencial by building the layer with dropouts and batchnormalisation\n",
    "\n",
    "model_6 = Sequential()       \n",
    "model_6.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(x,y,z,3),padding='same'))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(Activation('relu'))\n",
    "\n",
    "model_6.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "model_6.add(Activation('relu'))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_6.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_6.add(Activation('relu'))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_6.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_6.add(Activation('relu'))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_6.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_6.add(Activation('relu'))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_6.add(Flatten())\n",
    "\n",
    "model_6.add(Dense(1000, activation='relu'))\n",
    "model_6.add(Dropout(0.5))\n",
    "\n",
    "model_6.add(Dense(500, activation='relu'))\n",
    "model_6.add(Dropout(0.5))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_6.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_6.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train and validate the model \n",
    "batch_size = 50\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Commenting out as it was causing OOM error.\n",
    "\n",
    "#model_6.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "#                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "#                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "***ResourceExhaustedError: OOM when allocating tensor with shape[50,16,30,120,120] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc***\n",
    "\n",
    "- ***Best Training Accuracy - None %***\n",
    "- ***Best Validation Accuracy - None %***\n",
    "\n",
    "By increasing the memory the tensor size increases and the GPU outputs results in OOM exception, the resources are not enough to run the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - 7 : Hyperparameter Tuned : Batch Size (Decreasing to 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 15\n",
    "- Image Height : 120\n",
    "- Image Width : 120\n",
    "- Epochs - 25\n",
    "- Optimizer - Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "17/17 [==============================] - 38s 2s/step - loss: 10.4041 - categorical_accuracy: 0.2510 - val_loss: 11.8199 - val_categorical_accuracy: 0.2667\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0709_59_17.824524/model-00001-10.40410-0.25098-11.81994-0.26667.h5\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - 30s 2s/step - loss: 11.1138 - categorical_accuracy: 0.2706 - val_loss: 12.5363 - val_categorical_accuracy: 0.2222\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0709_59_17.824524/model-00002-11.11381-0.27059-12.53630-0.22222.h5\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - 24s 1s/step - loss: 8.7952 - categorical_accuracy: 0.4194 - val_loss: 12.8945 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0709_59_17.824524/model-00003-8.92132-0.41520-12.89448-0.20000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - 5s 318ms/step - loss: 9.4955 - categorical_accuracy: 0.3922 - val_loss: 7.0049 - val_categorical_accuracy: 0.5667\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0709_59_17.824524/model-00004-9.49549-0.39216-7.00489-0.56667.h5\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - 7s 416ms/step - loss: 11.1909 - categorical_accuracy: 0.2941 - val_loss: 12.6367 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0709_59_17.824524/model-00005-11.19090-0.29412-12.63675-0.20000.h5\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - 7s 403ms/step - loss: 11.3775 - categorical_accuracy: 0.2941 - val_loss: 13.4848 - val_categorical_accuracy: 0.1333\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0709_59_17.824524/model-00006-11.37748-0.29412-13.48479-0.13333.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - 7s 392ms/step - loss: 11.6935 - categorical_accuracy: 0.2745 - val_loss: 10.7468 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0709_59_17.824524/model-00007-11.69352-0.27451-10.74683-0.33333.h5\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - 7s 386ms/step - loss: 12.3257 - categorical_accuracy: 0.2353 - val_loss: 10.7454 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0709_59_17.824524/model-00008-12.32566-0.23529-10.74540-0.33333.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - 7s 390ms/step - loss: 10.9381 - categorical_accuracy: 0.3137 - val_loss: 9.3995 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0709_59_17.824524/model-00009-10.93813-0.31373-9.39950-0.40000.h5\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - 7s 399ms/step - loss: 8.7419 - categorical_accuracy: 0.4510 - val_loss: 11.5998 - val_categorical_accuracy: 0.2667\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0709_59_17.824524/model-00010-8.74190-0.45098-11.59982-0.26667.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - 8s 453ms/step - loss: 10.0868 - categorical_accuracy: 0.3529 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0709_59_17.824524/model-00011-10.08683-0.35294-9.67088-0.40000.h5\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - 6s 374ms/step - loss: 10.2771 - categorical_accuracy: 0.3529 - val_loss: 8.5963 - val_categorical_accuracy: 0.4667\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0709_59_17.824524/model-00012-10.27713-0.35294-8.59632-0.46667.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - 7s 383ms/step - loss: 10.1133 - categorical_accuracy: 0.3725 - val_loss: 8.5963 - val_categorical_accuracy: 0.4667\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0709_59_17.824524/model-00013-10.11331-0.37255-8.59632-0.46667.h5\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - 6s 369ms/step - loss: 10.6540 - categorical_accuracy: 0.3333 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0709_59_17.824524/model-00014-10.65399-0.33333-9.67086-0.40000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - 7s 383ms/step - loss: 8.8709 - categorical_accuracy: 0.4314 - val_loss: 10.7454 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0709_59_17.824524/model-00015-8.87089-0.43137-10.74540-0.33333.h5\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - 7s 419ms/step - loss: 10.9842 - categorical_accuracy: 0.3137 - val_loss: 10.7454 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0709_59_17.824524/model-00016-10.98423-0.31373-10.74540-0.33333.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - 7s 387ms/step - loss: 11.0617 - categorical_accuracy: 0.3137 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0709_59_17.824524/model-00017-11.06172-0.31373-11.28267-0.30000.h5\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - 7s 399ms/step - loss: 8.8772 - categorical_accuracy: 0.4314 - val_loss: 8.0590 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0709_59_17.824524/model-00018-8.87719-0.43137-8.05905-0.50000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - 6s 370ms/step - loss: 10.7454 - categorical_accuracy: 0.3333 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0709_59_17.824524/model-00019-10.74540-0.33333-9.67086-0.40000.h5\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - 7s 396ms/step - loss: 11.6936 - categorical_accuracy: 0.2745 - val_loss: 10.7454 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0709_59_17.824524/model-00020-11.69360-0.27451-10.74540-0.33333.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - 7s 397ms/step - loss: 10.7754 - categorical_accuracy: 0.2941 - val_loss: 9.1336 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-02-0709_59_17.824524/model-00021-10.77543-0.29412-9.13359-0.43333.h5\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - 7s 385ms/step - loss: 10.7454 - categorical_accuracy: 0.3333 - val_loss: 8.5963 - val_categorical_accuracy: 0.4667\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-02-0709_59_17.824524/model-00022-10.74540-0.33333-8.59632-0.46667.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - 7s 424ms/step - loss: 10.4294 - categorical_accuracy: 0.3529 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-02-0709_59_17.824524/model-00023-10.42936-0.35294-11.28267-0.30000.h5\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - 7s 419ms/step - loss: 10.4038 - categorical_accuracy: 0.3529 - val_loss: 8.0590 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-02-0709_59_17.824524/model-00024-10.40383-0.35294-8.05905-0.50000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - 7s 408ms/step - loss: 9.1652 - categorical_accuracy: 0.4314 - val_loss: 10.2081 - val_categorical_accuracy: 0.3667\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-02-0709_59_17.824524/model-00025-9.16519-0.43137-10.20813-0.36667.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a6f694f98>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us train and validate the model \n",
    "batch_size = 15\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "##Reusing model-6 as the overall architecture remains the same but only the batch_size is changed.\n",
    "model_6.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "\n",
    "- ***Best Training Accuracy - 43.14 %***\n",
    "- ***Best Validation Accuracy - 36.67 %***\n",
    "\n",
    "By decreasing the number of video sequences in each batch, i.e batch size the model is not able to learn the data, and the overall accuracy is very poor to be selected as the base model.\n",
    "\n",
    "The difference between training accuracy and validation accuracy is also very poor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture 2 - Conv2D + RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - 8 : Use of Conv2D + RNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 40\n",
    "- Image Height : 120\n",
    "- Image Width : 120\n",
    "- Epochs - 25\n",
    "- 4 Layers of Conv2D + LSTM + Dense + Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 30, 120, 120, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 60, 60, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 30, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 30, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 30, 30, 30, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 30, 30, 30, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 30, 15, 15, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 30, 15, 15, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 30, 15, 15, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 30, 7, 7, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 30, 6272)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                1622272   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,725,157\n",
      "Trainable params: 1,724,677\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let us import all the needed libraries of Keras.\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "# Let us experiment different x,y,z value in the CNNLSTM network and find tune all the image size & Hyperparameters later\n",
    "\n",
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "# Input all the images sequencial by building the layer with dropouts and batchnormalisation\n",
    "\n",
    "model_8 = Sequential()   \n",
    "model_8.add(TimeDistributed(Conv2D(16, (3, 3),padding='same', activation='relu'),input_shape=(x,y,z,3)))\n",
    "model_8.add(TimeDistributed(BatchNormalization()))\n",
    "model_8.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model_8.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model_8.add(TimeDistributed(BatchNormalization()))\n",
    "model_8.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model_8.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model_8.add(TimeDistributed(BatchNormalization()))\n",
    "model_8.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model_8.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model_8.add(TimeDistributed(BatchNormalization()))\n",
    "model_8.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_8.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_8.add(LSTM(64))\n",
    "model_8.add(Dropout(0.25))\n",
    "\n",
    "# Dense layer \n",
    "model_8.add(Dense(64,activation='relu'))\n",
    "model_8.add(Dropout(0.25))\n",
    "\n",
    "# Softmax layer\n",
    "model_8.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Adam optimiser\n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_8.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_8.summary())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train and validate the model \n",
    "batch_size = 40\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "17/17 [==============================] - 82s 5s/step - loss: 1.5270 - categorical_accuracy: 0.3182 - val_loss: 1.3460 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0709_59_17.824524/model-00001-1.53326-0.31222-1.34602-0.46000.h5\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 1.3073 - categorical_accuracy: 0.4501 - val_loss: 1.3248 - val_categorical_accuracy: 0.4167\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0709_59_17.824524/model-00002-1.30735-0.45013-1.32478-0.41667.h5\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - 41s 2s/step - loss: 1.2284 - categorical_accuracy: 0.4982 - val_loss: 1.3049 - val_categorical_accuracy: 0.5167\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0709_59_17.824524/model-00003-1.22884-0.50136-1.30487-0.51667.h5\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - 37s 2s/step - loss: 1.1872 - categorical_accuracy: 0.5015 - val_loss: 1.6664 - val_categorical_accuracy: 0.3167\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0709_59_17.824524/model-00004-1.18725-0.50155-1.66637-0.31667.h5\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 1.1808 - categorical_accuracy: 0.4935 - val_loss: 1.3509 - val_categorical_accuracy: 0.4167\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0709_59_17.824524/model-00005-1.18328-0.49201-1.35089-0.41667.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - 31s 2s/step - loss: 1.0675 - categorical_accuracy: 0.5917 - val_loss: 1.1717 - val_categorical_accuracy: 0.5667\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0709_59_17.824524/model-00006-1.06754-0.59170-1.17173-0.56667.h5\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 1.0087 - categorical_accuracy: 0.6194 - val_loss: 0.9612 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0709_59_17.824524/model-00007-1.00869-0.61938-0.96120-0.61667.h5\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.9594 - categorical_accuracy: 0.6228 - val_loss: 1.1652 - val_categorical_accuracy: 0.5167\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0709_59_17.824524/model-00008-0.95936-0.62284-1.16522-0.51667.h5\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.9544 - categorical_accuracy: 0.6505 - val_loss: 1.0234 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0709_59_17.824524/model-00009-0.95437-0.65052-1.02340-0.58333.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.9329 - categorical_accuracy: 0.6401 - val_loss: 0.9846 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0709_59_17.824524/model-00010-0.93287-0.64014-0.98463-0.65000.h5\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.7862 - categorical_accuracy: 0.7301 - val_loss: 1.0030 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0709_59_17.824524/model-00011-0.78618-0.73010-1.00298-0.61667.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.8430 - categorical_accuracy: 0.6817 - val_loss: 0.8484 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0709_59_17.824524/model-00012-0.84297-0.68166-0.84836-0.68333.h5\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.7427 - categorical_accuracy: 0.7474 - val_loss: 1.0127 - val_categorical_accuracy: 0.5667\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0709_59_17.824524/model-00013-0.74274-0.74740-1.01270-0.56667.h5\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.7542 - categorical_accuracy: 0.7197 - val_loss: 0.9191 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0709_59_17.824524/model-00014-0.75418-0.71972-0.91907-0.60000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.7187 - categorical_accuracy: 0.7405 - val_loss: 0.9546 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0709_59_17.824524/model-00015-0.71871-0.74048-0.95458-0.58333.h5\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.6574 - categorical_accuracy: 0.7578 - val_loss: 0.8884 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0709_59_17.824524/model-00016-0.65738-0.75779-0.88837-0.70000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - 30s 2s/step - loss: 0.6602 - categorical_accuracy: 0.7682 - val_loss: 0.9892 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0709_59_17.824524/model-00017-0.66020-0.76817-0.98923-0.60000.h5\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.6852 - categorical_accuracy: 0.7578 - val_loss: 0.7642 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0709_59_17.824524/model-00018-0.68525-0.75779-0.76422-0.68333.h5\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.7052 - categorical_accuracy: 0.7301 - val_loss: 0.9186 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0709_59_17.824524/model-00019-0.70516-0.73010-0.91864-0.65000.h5\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.7174 - categorical_accuracy: 0.7578 - val_loss: 0.9413 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0709_59_17.824524/model-00020-0.71736-0.75779-0.94128-0.61667.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.6645 - categorical_accuracy: 0.7889 - val_loss: 0.8161 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-02-0709_59_17.824524/model-00021-0.66452-0.78893-0.81609-0.66667.h5\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.6587 - categorical_accuracy: 0.7889 - val_loss: 0.8605 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-02-0709_59_17.824524/model-00022-0.65875-0.78893-0.86054-0.68333.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.6413 - categorical_accuracy: 0.7924 - val_loss: 0.8655 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-02-0709_59_17.824524/model-00023-0.64134-0.79239-0.86546-0.61667.h5\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.7218 - categorical_accuracy: 0.7682 - val_loss: 0.8561 - val_categorical_accuracy: 0.6333\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-02-0709_59_17.824524/model-00024-0.72180-0.76817-0.85607-0.63333.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.6181 - categorical_accuracy: 0.7785 - val_loss: 1.0027 - val_categorical_accuracy: 0.5333\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-02-0709_59_17.824524/model-00025-0.61815-0.77855-1.00266-0.53333.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a6f6c1630>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us fit the model\n",
    "\n",
    "model_8.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "\n",
    "- ***Best Training Accuracy - 78.89 %***\n",
    "- ***Best Validation Accuracy - 68.33 %***\n",
    "\n",
    "The above training accuracy is decent but fails in case of validation accuracy, this could be because the LSTM model used is very simple and introduction of more dense layers can help. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-9 : With GRU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 40\n",
    "- Image Height : 120\n",
    "- Image Width : 120\n",
    "- Epochs - 25\n",
    "- 4 Layers of Conv2D + GRU + Dense + Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_14 (TimeDis (None, 30, 120, 120, 8)   224       \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 30, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 30, 60, 60, 16)    1168      \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 30, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 30, 30, 30, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 30, 30, 30, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 30, 30, 30, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 30, 15, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 30, 15, 15, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 30, 15, 15, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 30, 7, 7, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 30, 7, 7, 128)     73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, 30, 7, 7, 128)     512       \n",
      "_________________________________________________________________\n",
      "time_distributed_28 (TimeDis (None, 30, 3, 3, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, 30, 1152)          0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64)                233664    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 337,525\n",
      "Trainable params: 337,029\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let us experiment different x,y,z value in the CNNLSTM network and find tune all the image size & Hyperparameters later\n",
    "\n",
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "# Input all the images sequencial by building the layer with dropouts and batchnormalisation\n",
    "\n",
    "model_9 = Sequential()   \n",
    "model_9.add(TimeDistributed(Conv2D(8, (3, 3),padding='same', activation='relu'),input_shape=(x,y,z,3)))\n",
    "model_9.add(TimeDistributed(BatchNormalization()))\n",
    "model_9.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model_9.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu')))\n",
    "model_9.add(TimeDistributed(BatchNormalization()))\n",
    "model_9.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model_9.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model_9.add(TimeDistributed(BatchNormalization()))\n",
    "model_9.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model_9.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model_9.add(TimeDistributed(BatchNormalization()))\n",
    "model_9.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model_9.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model_9.add(TimeDistributed(BatchNormalization()))\n",
    "model_9.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_9.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_9.add(GRU(64))\n",
    "model_9.add(Dropout(0.25))\n",
    "\n",
    "# Dense layer \n",
    "model_9.add(Dense(64,activation='relu'))\n",
    "model_9.add(Dropout(0.25))\n",
    "\n",
    "# Softmax layer\n",
    "model_9.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Adam optimiser\n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_9.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_9.summary())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train and validate the model \n",
    "batch_size = 40\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "17/17 [==============================] - 83s 5s/step - loss: 1.4588 - categorical_accuracy: 0.3987 - val_loss: 1.2502 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0709_59_17.824524/model-00001-1.46695-0.39367-1.25016-0.52000.h5\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 1.1018 - categorical_accuracy: 0.5780 - val_loss: 1.1080 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0709_59_17.824524/model-00002-1.10179-0.57801-1.10797-0.61667.h5\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - 41s 2s/step - loss: 1.0408 - categorical_accuracy: 0.5936 - val_loss: 1.2260 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0709_59_17.824524/model-00003-1.04822-0.58583-1.22596-0.50000.h5\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.9275 - categorical_accuracy: 0.6378 - val_loss: 0.9744 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0709_59_17.824524/model-00004-0.92755-0.63777-0.97438-0.60000.h5\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.9571 - categorical_accuracy: 0.6034 - val_loss: 0.9840 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0709_59_17.824524/model-00005-0.95843-0.60383-0.98401-0.60000.h5\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.8446 - categorical_accuracy: 0.6886 - val_loss: 1.0327 - val_categorical_accuracy: 0.5667\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0709_59_17.824524/model-00006-0.84464-0.68858-1.03267-0.56667.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.8472 - categorical_accuracy: 0.6436 - val_loss: 0.8456 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0709_59_17.824524/model-00007-0.84717-0.64360-0.84559-0.73333.h5\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.6087 - categorical_accuracy: 0.7785 - val_loss: 1.0168 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0709_59_17.824524/model-00008-0.60866-0.77855-1.01684-0.58333.h5\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.6559 - categorical_accuracy: 0.7820 - val_loss: 0.8995 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0709_59_17.824524/model-00009-0.65587-0.78201-0.89946-0.55000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.5949 - categorical_accuracy: 0.7682 - val_loss: 0.9504 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0709_59_17.824524/model-00010-0.59495-0.76817-0.95044-0.65000.h5\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.4822 - categorical_accuracy: 0.8374 - val_loss: 0.8312 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0709_59_17.824524/model-00011-0.48224-0.83737-0.83116-0.60000.h5\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.3999 - categorical_accuracy: 0.8651 - val_loss: 0.9290 - val_categorical_accuracy: 0.6333\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0709_59_17.824524/model-00012-0.39993-0.86505-0.92899-0.63333.h5\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.3537 - categorical_accuracy: 0.8893 - val_loss: 0.7729 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0709_59_17.824524/model-00013-0.35367-0.88927-0.77286-0.65000.h5\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.4175 - categorical_accuracy: 0.8824 - val_loss: 0.9753 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0709_59_17.824524/model-00014-0.41746-0.88235-0.97528-0.61667.h5\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.3256 - categorical_accuracy: 0.9066 - val_loss: 0.9284 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0709_59_17.824524/model-00015-0.32555-0.90657-0.92836-0.66667.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.3032 - categorical_accuracy: 0.9135 - val_loss: 0.9878 - val_categorical_accuracy: 0.6333\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0709_59_17.824524/model-00016-0.30317-0.91349-0.98781-0.63333.h5\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.2851 - categorical_accuracy: 0.9170 - val_loss: 1.0409 - val_categorical_accuracy: 0.5667\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0709_59_17.824524/model-00017-0.28512-0.91696-1.04095-0.56667.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.2298 - categorical_accuracy: 0.9343 - val_loss: 0.6883 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0709_59_17.824524/model-00018-0.22981-0.93426-0.68828-0.71667.h5\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.2252 - categorical_accuracy: 0.9619 - val_loss: 1.0241 - val_categorical_accuracy: 0.5667\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0709_59_17.824524/model-00019-0.22517-0.96194-1.02406-0.56667.h5\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.2355 - categorical_accuracy: 0.9516 - val_loss: 0.9040 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0709_59_17.824524/model-00020-0.23554-0.95156-0.90400-0.66667.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.2578 - categorical_accuracy: 0.9343 - val_loss: 0.8316 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-02-0709_59_17.824524/model-00021-0.25783-0.93426-0.83155-0.66667.h5\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.2269 - categorical_accuracy: 0.9308 - val_loss: 0.9920 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-02-0709_59_17.824524/model-00022-0.22694-0.93080-0.99205-0.60000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.2256 - categorical_accuracy: 0.9550 - val_loss: 0.8431 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-02-0709_59_17.824524/model-00023-0.22556-0.95502-0.84306-0.68333.h5\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.2029 - categorical_accuracy: 0.9689 - val_loss: 0.7716 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-02-0709_59_17.824524/model-00024-0.20289-0.96886-0.77164-0.70000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.2545 - categorical_accuracy: 0.9308 - val_loss: 0.8238 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-02-0709_59_17.824524/model-00025-0.25447-0.93080-0.82376-0.70000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a65472390>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_9.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "\n",
    "- ***Best Training Accuracy - 93.08 %***\n",
    "- ***Best Validation Accuracy - 70.00 %***\n",
    "\n",
    "The difference between Training and validation accuracy is very huge (around 20%) and this indicates Overfitting i.e the model does not fit well for unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - 10 : Hyper parameter tuned on Base Model (Model 3) - Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 40\n",
    "- Image Height : 120\n",
    "- Image Width : 120\n",
    "- Epochs - 25\n",
    "- Conv3D \n",
    "- Cropping and Data Augmentation on Input data - Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let we do the generators and input the images as we see that our images have two different sizes. \n",
    "x = 30 # No. of frames images\n",
    "y = 120 # Width of the image\n",
    "z = 120 # height\n",
    "\n",
    "def generatorWithAugmentation(source_path, folder_list, batch_size):\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    # Let us resize all the images \n",
    "                    \n",
    "                    shifted = cv2.warpAffine(image, np.float32([[1, 0, np.random.randint(-20,20)],[0, 1, np.random.randint(-20,20)]]),(image.shape[1], image.shape[0]))\n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    \n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    resized_image = imresize(cropped,(y,z))\n",
    "                    resized_image = resized_image/255\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (resized_image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (resized_image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (resized_image[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches        \n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    shifted = cv2.warpAffine(image, np.float32([[1, 0, np.random.randint(-20,20)],[0, 1, np.random.randint(-20,20)]]),(image.shape[1], image.shape[0]))\n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    \n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    resized_image = imresize(cropped,(y,z))\n",
    "                    resized_image = resized_image/255\n",
    "                                        \n",
    "                    batch_data[folder,idx,:,:,0] = (resized_image[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (resized_image[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (resized_image[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us import all the needed libraries of Keras.\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 30, 120, 120, 16)  3472      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 15, 60, 60, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 7, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 3, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              6273000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_10 = Sequential()       \n",
    "model_10.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(x,y,z,3),padding='same'))\n",
    "model_10.add(BatchNormalization())\n",
    "model_10.add(Activation('relu'))\n",
    "\n",
    "model_10.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "model_10.add(Activation('relu'))\n",
    "model_10.add(BatchNormalization())\n",
    "model_10.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_10.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_10.add(Activation('relu'))\n",
    "model_10.add(BatchNormalization())\n",
    "model_10.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_10.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_10.add(Activation('relu'))\n",
    "model_10.add(BatchNormalization())\n",
    "model_10.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_10.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_10.add(Activation('relu'))\n",
    "model_10.add(BatchNormalization())\n",
    "model_10.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_10.add(Flatten())\n",
    "\n",
    "model_10.add(Dense(1000, activation='relu'))\n",
    "model_10.add(Dropout(0.5))\n",
    "\n",
    "model_10.add(Dense(500, activation='relu'))\n",
    "model_10.add(Dropout(0.55))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_10.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_10.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see the Validate the Losses and put back the checkpoint\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see that the steps_per_epoch and validation steps are used by fit_generator to decide the no. of next()\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generatorWithAugmentation(train_path, train_doc, batch_size)\n",
    "val_generator = generatorWithAugmentation(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "17/17 [==============================] - 204s 12s/step - loss: 8.6946 - categorical_accuracy: 0.3017 - val_loss: 7.3340 - val_categorical_accuracy: 0.4833\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0714_43_09.902843/model-00001-8.76739-0.29575-7.33396-0.48333.h5\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - 72s 4s/step - loss: 7.3420 - categorical_accuracy: 0.4041 - val_loss: 7.5635 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0714_43_09.902843/model-00002-7.34197-0.40409-7.56350-0.43333.h5\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - 63s 4s/step - loss: 6.5572 - categorical_accuracy: 0.4537 - val_loss: 6.6065 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0714_43_09.902843/model-00003-6.57397-0.45352-6.60649-0.35000.h5\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - 60s 4s/step - loss: 5.1854 - categorical_accuracy: 0.4180 - val_loss: 4.9274 - val_categorical_accuracy: 0.3833\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0714_43_09.902843/model-00004-5.18537-0.41796-4.92736-0.38333.h5\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - 54s 3s/step - loss: 3.6576 - categorical_accuracy: 0.4867 - val_loss: 7.0571 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0714_43_09.902843/model-00005-3.67416-0.48534-7.05714-0.40000.h5\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - 52s 3s/step - loss: 4.1234 - categorical_accuracy: 0.4118 - val_loss: 3.7968 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0714_43_09.902843/model-00006-4.12345-0.41176-3.79681-0.43333.h5\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - 51s 3s/step - loss: 2.6779 - categorical_accuracy: 0.5363 - val_loss: 3.5296 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0714_43_09.902843/model-00007-2.67785-0.53633-3.52961-0.43333.h5\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - 52s 3s/step - loss: 2.0757 - categorical_accuracy: 0.5848 - val_loss: 2.4775 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0714_43_09.902843/model-00008-2.07575-0.58478-2.47751-0.60000.h5\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - 50s 3s/step - loss: 1.9895 - categorical_accuracy: 0.5502 - val_loss: 2.8253 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0714_43_09.902843/model-00009-1.98947-0.55017-2.82533-0.43333.h5\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - 52s 3s/step - loss: 1.7493 - categorical_accuracy: 0.5779 - val_loss: 2.4666 - val_categorical_accuracy: 0.4333\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0714_43_09.902843/model-00010-1.74932-0.57785-2.46660-0.43333.h5\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - 52s 3s/step - loss: 1.5537 - categorical_accuracy: 0.5952 - val_loss: 3.1614 - val_categorical_accuracy: 0.4667\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0714_43_09.902843/model-00011-1.55367-0.59516-3.16138-0.46667.h5\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - 51s 3s/step - loss: 1.4658 - categorical_accuracy: 0.5848 - val_loss: 4.2809 - val_categorical_accuracy: 0.3667\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0714_43_09.902843/model-00012-1.46577-0.58478-4.28086-0.36667.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - 52s 3s/step - loss: 1.1704 - categorical_accuracy: 0.6574 - val_loss: 3.8236 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0714_43_09.902843/model-00013-1.17038-0.65744-3.82358-0.33333.h5\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - 50s 3s/step - loss: 1.0532 - categorical_accuracy: 0.6436 - val_loss: 0.8398 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0714_43_09.902843/model-00014-1.05323-0.64360-0.83980-0.66667.h5\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - 53s 3s/step - loss: 0.8034 - categorical_accuracy: 0.7578 - val_loss: 1.0797 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0714_43_09.902843/model-00015-0.80339-0.75779-1.07972-0.65000.h5\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.8952 - categorical_accuracy: 0.7301 - val_loss: 0.6566 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0714_43_09.902843/model-00016-0.89515-0.73010-0.65663-0.80000.h5\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.7691 - categorical_accuracy: 0.7197 - val_loss: 0.7570 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0714_43_09.902843/model-00017-0.76907-0.71972-0.75702-0.70000.h5\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - 53s 3s/step - loss: 0.6813 - categorical_accuracy: 0.7889 - val_loss: 0.7996 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0714_43_09.902843/model-00018-0.68126-0.78893-0.79962-0.73333.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.5530 - categorical_accuracy: 0.7924 - val_loss: 0.9676 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0714_43_09.902843/model-00019-0.55299-0.79239-0.96764-0.66667.h5\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.5995 - categorical_accuracy: 0.7682 - val_loss: 0.8137 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0714_43_09.902843/model-00020-0.59945-0.76817-0.81371-0.75000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.5212 - categorical_accuracy: 0.7993 - val_loss: 0.7378 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-02-0714_43_09.902843/model-00021-0.52117-0.79931-0.73779-0.71667.h5\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - 52s 3s/step - loss: 0.5255 - categorical_accuracy: 0.8201 - val_loss: 0.8078 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-02-0714_43_09.902843/model-00022-0.52552-0.82007-0.80782-0.68333.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - 53s 3s/step - loss: 0.4481 - categorical_accuracy: 0.8685 - val_loss: 0.7077 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-02-0714_43_09.902843/model-00023-0.44812-0.86851-0.70768-0.66667.h5\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.4214 - categorical_accuracy: 0.8201 - val_loss: 0.6662 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-02-0714_43_09.902843/model-00024-0.42135-0.82007-0.66620-0.73333.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.4657 - categorical_accuracy: 0.8166 - val_loss: 0.8758 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-02-0714_43_09.902843/model-00025-0.46568-0.81661-0.87581-0.73333.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe2ccd00a58>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us fit the model\n",
    "model_10.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "\n",
    "- ***Best Training Accuracy - 82.01 %***\n",
    "- ***Best Validation Accuracy - 73.33 %***\n",
    "\n",
    "The training accuracy increases gradually throughout the model but fluctuates highly for validation accuracy, and this indicates it is not a stable model and would not perform well on unseen data. \n",
    "\n",
    "Data augmentation increases the computation time and is not a suitable model. \n",
    "\n",
    "**Hence we continue with Model 3 as our Base Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 11 : Hyperparameter Tuned : Model Architecture - Added Dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 40\n",
    "- Image Height : 120\n",
    "- Image Width : 120\n",
    "- Epochs - 25\n",
    "- Conv3D - Add Dropouts at each Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train and validate the model \n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Added more dropouts, and each dropout value is changed from 0.5 to 0.25**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_6 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 30, 120, 120, 16)  3472      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 15, 60, 60, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 7, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 3, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              6273000   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_11 = Sequential()       \n",
    "model_11.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(x,y,z,3),padding='same'))\n",
    "model_11.add(BatchNormalization())\n",
    "model_11.add(Activation('relu'))\n",
    "\n",
    "model_11.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "model_11.add(Activation('relu'))\n",
    "model_11.add(BatchNormalization())\n",
    "model_11.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_11.add(Dropout(0.25))\n",
    "\n",
    "model_11.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_11.add(Activation('relu'))\n",
    "model_11.add(BatchNormalization())\n",
    "model_11.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_11.add(Dropout(0.25))\n",
    "\n",
    "model_11.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_11.add(Activation('relu'))\n",
    "model_11.add(BatchNormalization())\n",
    "model_11.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_11.add(Dropout(0.25))\n",
    "\n",
    "model_11.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_11.add(Activation('relu'))\n",
    "model_11.add(BatchNormalization())\n",
    "model_11.add(MaxPooling3D(pool_size=(2, 2, 2)))    \n",
    "model_11.add(Dropout(0.25))  \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_11.add(Flatten())\n",
    "\n",
    "model_11.add(Dense(1000, activation='relu'))\n",
    "model_11.add(Dropout(0.25))\n",
    "\n",
    "model_11.add(Dense(500, activation='relu'))\n",
    "model_11.add(Dropout(0.25))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_11.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_11.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "17/17 [==============================] - 87s 5s/step - loss: 12.2285 - categorical_accuracy: 0.2104 - val_loss: 12.2498 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0714_43_09.902843/model-00001-12.25312-0.20814-12.24975-0.24000.h5\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - 44s 3s/step - loss: 13.1501 - categorical_accuracy: 0.1841 - val_loss: 12.8945 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0714_43_09.902843/model-00002-13.15006-0.18414-12.89448-0.20000.h5\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - 43s 3s/step - loss: 12.7935 - categorical_accuracy: 0.2063 - val_loss: 12.3456 - val_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0714_43_09.902843/model-00003-12.78029-0.20708-12.34559-0.23333.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - 40s 2s/step - loss: 13.1240 - categorical_accuracy: 0.1858 - val_loss: 12.0886 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0714_43_09.902843/model-00004-13.12402-0.18576-12.08857-0.25000.h5\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - 38s 2s/step - loss: 12.5246 - categorical_accuracy: 0.2230 - val_loss: 12.8945 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0714_43_09.902843/model-00005-12.51341-0.22364-12.89448-0.20000.h5\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 13.3853 - categorical_accuracy: 0.1696 - val_loss: 12.5655 - val_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0714_43_09.902843/model-00006-13.38527-0.16955-12.56553-0.21667.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 12.3814 - categorical_accuracy: 0.2318 - val_loss: 13.0945 - val_categorical_accuracy: 0.1833\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0714_43_09.902843/model-00007-12.38137-0.23183-13.09451-0.18333.h5\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 13.0506 - categorical_accuracy: 0.1903 - val_loss: 11.8199 - val_categorical_accuracy: 0.2667\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0714_43_09.902843/model-00008-13.05064-0.19031-11.81994-0.26667.h5\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 13.0506 - categorical_accuracy: 0.1903 - val_loss: 13.0894 - val_categorical_accuracy: 0.1833\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0714_43_09.902843/model-00009-13.05064-0.19031-13.08940-0.18333.h5\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 12.6602 - categorical_accuracy: 0.2145 - val_loss: 11.8199 - val_categorical_accuracy: 0.2667\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0714_43_09.902843/model-00010-12.66023-0.21453-11.81994-0.26667.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 12.9949 - categorical_accuracy: 0.1938 - val_loss: 12.8178 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0714_43_09.902843/model-00011-12.99487-0.19377-12.81781-0.20000.h5\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 12.8833 - categorical_accuracy: 0.2007 - val_loss: 13.4317 - val_categorical_accuracy: 0.1667\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0714_43_09.902843/model-00012-12.88332-0.20069-13.43175-0.16667.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 12.9949 - categorical_accuracy: 0.1938 - val_loss: 12.2798 - val_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0714_43_09.902843/model-00013-12.99487-0.19377-12.27981-0.23333.h5\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 13.0506 - categorical_accuracy: 0.1903 - val_loss: 12.2789 - val_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0714_43_09.902843/model-00014-13.05064-0.19031-12.27889-0.23333.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 12.9949 - categorical_accuracy: 0.1938 - val_loss: 12.5473 - val_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0714_43_09.902843/model-00015-12.99487-0.19377-12.54729-0.21667.h5\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 12.9391 - categorical_accuracy: 0.1972 - val_loss: 12.0886 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0714_43_09.902843/model-00016-12.93909-0.19723-12.08857-0.25000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 13.1622 - categorical_accuracy: 0.1834 - val_loss: 12.2786 - val_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0714_43_09.902843/model-00017-13.16218-0.18339-12.27861-0.23333.h5\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 12.6045 - categorical_accuracy: 0.2180 - val_loss: 12.3572 - val_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0714_43_09.902843/model-00018-12.60446-0.21799-12.35721-0.23333.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 13.6084 - categorical_accuracy: 0.1557 - val_loss: 12.8151 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0714_43_09.902843/model-00019-13.60836-0.15571-12.81507-0.20000.h5\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 12.5487 - categorical_accuracy: 0.2215 - val_loss: 13.4317 - val_categorical_accuracy: 0.1667\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0714_43_09.902843/model-00020-12.54869-0.22145-13.43175-0.16667.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 12.8833 - categorical_accuracy: 0.2007 - val_loss: 11.7402 - val_categorical_accuracy: 0.2667\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-02-0714_43_09.902843/model-00021-12.88332-0.20069-11.74016-0.26667.h5\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 12.9949 - categorical_accuracy: 0.1938 - val_loss: 12.2765 - val_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-02-0714_43_09.902843/model-00022-12.99487-0.19377-12.27654-0.23333.h5\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 12.9391 - categorical_accuracy: 0.1972 - val_loss: 12.6258 - val_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-02-0714_43_09.902843/model-00023-12.93909-0.19723-12.62584-0.21667.h5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 12.7718 - categorical_accuracy: 0.2076 - val_loss: 12.5452 - val_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-02-0714_43_09.902843/model-00024-12.77178-0.20761-12.54519-0.21667.h5\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 13.2180 - categorical_accuracy: 0.1799 - val_loss: 12.8138 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-02-0714_43_09.902843/model-00025-13.21795-0.17993-12.81379-0.20000.h5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe36881fcf8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us fit the model\n",
    "model_11.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "\n",
    "- ***Best Training Accuracy - 20.76 % approx***\n",
    "- ***Best Validation Accuracy - 20.67 % approx***\n",
    "\n",
    "The dropout layers increase the number of parameters that are assigned as 0 (dropping out 25% neurons in most of layers) and this underfits the model, as a result the model does not learn properly and it leads to a very poor accuracy.\n",
    "\n",
    "**Hence we continue with Model 3 as our Base Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 12 : Hyperparameter Tuned : Model Architecture - Added more dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 30, 120, 120, 16)  3472      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 15, 60, 60, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 15, 60, 60, 32)    8224      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 7, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 7, 30, 30, 64)     32832     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 3, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 3, 15, 15, 128)    131200    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              6273000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 7,040,517\n",
      "Trainable params: 7,039,573\n",
      "Non-trainable params: 944\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_12 = Sequential()       \n",
    "model_12.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(x,y,z,3),padding='same'))\n",
    "model_12.add(BatchNormalization())\n",
    "model_12.add(Activation('relu'))\n",
    "\n",
    "model_12.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "model_12.add(Activation('relu'))\n",
    "model_12.add(BatchNormalization())\n",
    "model_12.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_12.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_12.add(Activation('relu'))\n",
    "model_12.add(BatchNormalization())\n",
    "\n",
    "model_12.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_12.add(Activation('relu'))\n",
    "model_12.add(BatchNormalization())\n",
    "model_12.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_12.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_12.add(Activation('relu'))\n",
    "model_12.add(BatchNormalization())\n",
    "\n",
    "model_12.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_12.add(Activation('relu'))\n",
    "model_12.add(BatchNormalization())\n",
    "model_12.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_12.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_12.add(Activation('relu'))\n",
    "model_12.add(BatchNormalization())\n",
    "\n",
    "model_12.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_12.add(Activation('relu'))\n",
    "model_12.add(BatchNormalization())\n",
    "model_12.add(MaxPooling3D(pool_size=(2, 2, 2)))    \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_12.add(Flatten())\n",
    "\n",
    "model_12.add(Dense(1000, activation='relu'))\n",
    "model_12.add(Dropout(0.25))\n",
    "\n",
    "model_12.add(Dense(500, activation='relu'))\n",
    "model_12.add(Dropout(0.25))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_12.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_12.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_12.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "17/17 [==============================] - 100s 6s/step - loss: 11.4733 - categorical_accuracy: 0.2176 - val_loss: 13.0557 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0716_33_19.139208/model-00001-11.42872-0.21870-13.05566-0.19000.h5\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - 39s 2s/step - loss: 11.2100 - categorical_accuracy: 0.2967 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0716_33_19.139208/model-00002-11.20997-0.29668-11.28267-0.30000.h5\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - 47s 3s/step - loss: 11.4935 - categorical_accuracy: 0.2735 - val_loss: 12.6258 - val_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0716_33_19.139208/model-00003-11.52736-0.27248-12.62584-0.21667.h5\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 11.2095 - categorical_accuracy: 0.2972 - val_loss: 12.6258 - val_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0716_33_19.139208/model-00004-11.20947-0.29721-12.62584-0.21667.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - 41s 2s/step - loss: 10.3309 - categorical_accuracy: 0.3501 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0716_33_19.139208/model-00005-10.36683-0.34824-11.28267-0.30000.h5\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 10.0514 - categorical_accuracy: 0.3529 - val_loss: 10.8461 - val_categorical_accuracy: 0.2667\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0716_33_19.139208/model-00006-10.05143-0.35294-10.84612-0.26667.h5\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 10.7508 - categorical_accuracy: 0.3080 - val_loss: 12.2341 - val_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0716_33_19.139208/model-00007-10.75083-0.30796-12.23412-0.23333.h5\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 10.9428 - categorical_accuracy: 0.2872 - val_loss: 13.6822 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0716_33_19.139208/model-00008-10.94282-0.28720-13.68220-0.15000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 10.4787 - categorical_accuracy: 0.3183 - val_loss: 11.7776 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0716_33_19.139208/model-00009-10.47873-0.31834-11.77759-0.25000.h5\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 10.2062 - categorical_accuracy: 0.3218 - val_loss: 9.3475 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0716_33_19.139208/model-00010-10.20619-0.32180-9.34753-0.35000.h5\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 10.1070 - categorical_accuracy: 0.3495 - val_loss: 10.3062 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0716_33_19.139208/model-00011-10.10697-0.34948-10.30617-0.30000.h5\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - 37s 2s/step - loss: 9.4689 - categorical_accuracy: 0.3702 - val_loss: 11.1029 - val_categorical_accuracy: 0.2667\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0716_33_19.139208/model-00012-9.46894-0.37024-11.10288-0.26667.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 9.0511 - categorical_accuracy: 0.4048 - val_loss: 11.1693 - val_categorical_accuracy: 0.2667\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0716_33_19.139208/model-00013-9.05107-0.40484-11.16926-0.26667.h5\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 9.2108 - categorical_accuracy: 0.3841 - val_loss: 10.0068 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0716_33_19.139208/model-00014-9.21085-0.38408-10.00682-0.35000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 8.7227 - categorical_accuracy: 0.4325 - val_loss: 9.1473 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0716_33_19.139208/model-00015-8.72271-0.43253-9.14734-0.33333.h5\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 8.1468 - categorical_accuracy: 0.4567 - val_loss: 9.5298 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0716_33_19.139208/model-00016-8.14682-0.45675-9.52984-0.33333.h5\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 7.7485 - categorical_accuracy: 0.4740 - val_loss: 7.4508 - val_categorical_accuracy: 0.4833\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0716_33_19.139208/model-00017-7.74854-0.47405-7.45081-0.48333.h5\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 7.0363 - categorical_accuracy: 0.5225 - val_loss: 10.4478 - val_categorical_accuracy: 0.2667\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0716_33_19.139208/model-00018-7.03635-0.52249-10.44782-0.26667.h5\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 7.6097 - categorical_accuracy: 0.4740 - val_loss: 8.1342 - val_categorical_accuracy: 0.3833\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0716_33_19.139208/model-00019-7.60967-0.47405-8.13419-0.38333.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 6.8015 - categorical_accuracy: 0.5433 - val_loss: 8.0367 - val_categorical_accuracy: 0.4167\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0716_33_19.139208/model-00020-6.80148-0.54325-8.03666-0.41667.h5\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 7.6871 - categorical_accuracy: 0.4740 - val_loss: 7.8807 - val_categorical_accuracy: 0.4167\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-02-0716_33_19.139208/model-00021-7.68709-0.47405-7.88066-0.41667.h5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 7.4162 - categorical_accuracy: 0.5017 - val_loss: 8.2557 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-02-0716_33_19.139208/model-00022-7.41619-0.50173-8.25572-0.33333.h5\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 6.6431 - categorical_accuracy: 0.5433 - val_loss: 7.8782 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-02-0716_33_19.139208/model-00023-6.64308-0.54325-7.87822-0.45000.h5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 6.6834 - categorical_accuracy: 0.5190 - val_loss: 7.2872 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-02-0716_33_19.139208/model-00024-6.68340-0.51903-7.28719-0.45000.h5\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 7.7692 - categorical_accuracy: 0.4740 - val_loss: 8.3076 - val_categorical_accuracy: 0.3833\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-02-0716_33_19.139208/model-00025-7.76919-0.47405-8.30756-0.38333.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff43c3dbbe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us fit the model\n",
    "model_12.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "\n",
    "- ***Best Training Accuracy - 54.33 %***\n",
    "- ***Best Validation Accuracy - 45.00 %***\n",
    "\n",
    "The model is undefitting as well as complex.The foremost objective of training machine learning based model is to keep a good trade-off between simplicity of the model and the performance accuracy which is not achieved with this model.\n",
    "\n",
    "**Hence we continue with Model 3 as our Base Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - 13 : To reduce memory footprint of Model 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 40\n",
    "- Image Height : 120\n",
    "- Image Width : 120\n",
    "- Epochs - 25\n",
    "- Currently, the avg parameters that need to be trained = 7 Million , Reducing the parameters help us achieve a better model with less number of training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_9 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 30, 120, 120, 16)  3472      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 15, 60, 60, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 7, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_13 (Conv3D)           (None, 3, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               1605888   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 1,764,325\n",
      "Trainable params: 1,763,829\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let us experiment different x,y,z value in the CNN network and find tune all the image size & Hyperparameters later\n",
    "\n",
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "# Input all the images sequencial by building the layer with dropouts and batchnormalisation\n",
    "\n",
    "model_13 = Sequential()       \n",
    "model_13.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(x,y,z,3),padding='same'))\n",
    "model_13.add(BatchNormalization())\n",
    "model_13.add(Activation('relu'))\n",
    "\n",
    "model_13.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "model_13.add(Activation('relu'))\n",
    "model_13.add(BatchNormalization())\n",
    "model_13.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_13.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_13.add(Activation('relu'))\n",
    "model_13.add(BatchNormalization())\n",
    "model_13.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_13.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_13.add(Activation('relu'))\n",
    "model_13.add(BatchNormalization())\n",
    "model_13.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_13.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_13.add(Activation('relu'))\n",
    "model_13.add(BatchNormalization())\n",
    "model_13.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_13.add(Flatten())\n",
    "\n",
    "model_13.add(Dense(256, activation='relu'))\n",
    "model_13.add(Dropout(0.25))\n",
    "\n",
    "model_13.add(Dense(256, activation='relu'))\n",
    "model_13.add(Dropout(0.25))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_13.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_13.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_13.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "17/17 [==============================] - 88s 5s/step - loss: 2.5797 - categorical_accuracy: 0.3593 - val_loss: 1.7434 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0716_53_40.973445/model-00001-2.60513-0.35747-1.74344-0.42000.h5\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - 43s 3s/step - loss: 1.1854 - categorical_accuracy: 0.5448 - val_loss: 2.0881 - val_categorical_accuracy: 0.2833\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0716_53_40.973445/model-00002-1.18538-0.54476-2.08811-0.28333.h5\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - 43s 3s/step - loss: 1.0185 - categorical_accuracy: 0.6054 - val_loss: 1.9072 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0716_53_40.973445/model-00003-1.02032-0.60218-1.90722-0.50000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.9305 - categorical_accuracy: 0.6656 - val_loss: 0.9383 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0716_53_40.973445/model-00004-0.93048-0.66563-0.93835-0.60000.h5\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.7563 - categorical_accuracy: 0.7278 - val_loss: 0.6173 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0716_53_40.973445/model-00005-0.75794-0.72843-0.61726-0.76667.h5\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.5356 - categorical_accuracy: 0.8062 - val_loss: 0.9113 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0716_53_40.973445/model-00006-0.53558-0.80623-0.91126-0.58333.h5\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.7538 - categorical_accuracy: 0.7474 - val_loss: 0.9216 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0716_53_40.973445/model-00007-0.75382-0.74740-0.92161-0.68333.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.5331 - categorical_accuracy: 0.8097 - val_loss: 0.7327 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0716_53_40.973445/model-00008-0.53310-0.80969-0.73267-0.71667.h5\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.4183 - categorical_accuracy: 0.8581 - val_loss: 0.6910 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0716_53_40.973445/model-00009-0.41834-0.85813-0.69104-0.75000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.4090 - categorical_accuracy: 0.8685 - val_loss: 0.5788 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0716_53_40.973445/model-00010-0.40896-0.86851-0.57876-0.80000.h5\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.2853 - categorical_accuracy: 0.8893 - val_loss: 0.6834 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0716_53_40.973445/model-00011-0.28525-0.88927-0.68339-0.73333.h5\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.2741 - categorical_accuracy: 0.9135 - val_loss: 0.6006 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0716_53_40.973445/model-00012-0.27408-0.91349-0.60063-0.75000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.2568 - categorical_accuracy: 0.9239 - val_loss: 0.6577 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0716_53_40.973445/model-00013-0.25680-0.92388-0.65766-0.76667.h5\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.3044 - categorical_accuracy: 0.9066 - val_loss: 0.3754 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0716_53_40.973445/model-00014-0.30437-0.90657-0.37542-0.85000.h5\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.2588 - categorical_accuracy: 0.9135 - val_loss: 0.5502 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0716_53_40.973445/model-00015-0.25880-0.91349-0.55020-0.75000.h5\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.2455 - categorical_accuracy: 0.9135 - val_loss: 0.7417 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0716_53_40.973445/model-00016-0.24553-0.91349-0.74171-0.68333.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.2348 - categorical_accuracy: 0.9239 - val_loss: 0.7362 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0716_53_40.973445/model-00017-0.23477-0.92388-0.73623-0.70000.h5\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.2281 - categorical_accuracy: 0.9239 - val_loss: 0.4029 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0716_53_40.973445/model-00018-0.22814-0.92388-0.40291-0.80000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.2264 - categorical_accuracy: 0.9100 - val_loss: 0.6483 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0716_53_40.973445/model-00019-0.22636-0.91003-0.64832-0.78333.h5\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.2199 - categorical_accuracy: 0.9135 - val_loss: 0.4447 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0716_53_40.973445/model-00020-0.21989-0.91349-0.44470-0.80000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.2013 - categorical_accuracy: 0.9481 - val_loss: 0.6385 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-02-0716_53_40.973445/model-00021-0.20135-0.94810-0.63853-0.76667.h5\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.1966 - categorical_accuracy: 0.9446 - val_loss: 0.5439 - val_categorical_accuracy: 0.8333\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-02-0716_53_40.973445/model-00022-0.19661-0.94464-0.54390-0.83333.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.1721 - categorical_accuracy: 0.9412 - val_loss: 0.6652 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-02-0716_53_40.973445/model-00023-0.17212-0.94118-0.66518-0.75000.h5\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.1965 - categorical_accuracy: 0.9377 - val_loss: 0.4909 - val_categorical_accuracy: 0.8167\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-02-0716_53_40.973445/model-00024-0.19653-0.93772-0.49090-0.81667.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.1819 - categorical_accuracy: 0.9516 - val_loss: 0.5942 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-02-0716_53_40.973445/model-00025-0.18186-0.95156-0.59417-0.78333.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff436a7b048>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us fit the model\n",
    "model_13.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "\n",
    "- ***Best Training Accuracy - 93.77 % approx***\n",
    "- ***Best Validation Accuracy - 81.67% approx***\n",
    "\n",
    "The model is overfitting as there is a huge difference between the training and validation accuracy.\n",
    "\n",
    "**Hence we continue with Model 3 as our Base Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 14 : Hyperparameter Tuned : Filter size and Dense neurons (128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Model Summary***\n",
    "\n",
    "- Batch Size : 40\n",
    "- Image Height : 120\n",
    "- Image Width : 120\n",
    "- Epochs - 25\n",
    "- Dense Neurons : 128 \n",
    "- Filter size = 2,2,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let we do the generators and input the images as we see that our images have two different sizes. \n",
    "x = 20 # No. of frames images\n",
    "y = 120 # Width of the image\n",
    "z = 120 # height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    # Let us resize all the images \n",
    "                    resized_image = imresize(image,(y,z))\n",
    "                    resized_image = resized_image/255\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (resized_image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (resized_image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (resized_image[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches        \n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    resized_image = imresize(image,(y,z))\n",
    "                    resized_image = resized_image/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (resized_image[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (resized_image[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (resized_image[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_29 (Conv3D)           (None, 20, 120, 120, 8)   200       \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 20, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 20, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_30 (Conv3D)           (None, 20, 120, 120, 16)  1040      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 20, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 20, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_21 (MaxPooling (None, 10, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_31 (Conv3D)           (None, 10, 60, 60, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 10, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 10, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_22 (MaxPooling (None, 5, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_32 (Conv3D)           (None, 5, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 5, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 5, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_23 (MaxPooling (None, 2, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_33 (Conv3D)           (None, 2, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 2, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 2, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 908,573\n",
      "Trainable params: 908,077\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let us experiment different x,y,z value in the CNN network and find tune all the image size & Hyperparameters later\n",
    "\n",
    "x = 20 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "# Input all the images sequencial by building the layer with dropouts and batchnormalisation\n",
    "\n",
    "model_14 = Sequential()       \n",
    "model_14.add(Conv3D(8,kernel_size=(2,2,2),input_shape=(x,y,z,3),padding='same'))\n",
    "model_14.add(BatchNormalization())\n",
    "model_14.add(Activation('relu'))\n",
    "\n",
    "model_14.add(Conv3D(16, (2, 2, 2), padding='same'))\n",
    "model_14.add(Activation('relu'))\n",
    "model_14.add(BatchNormalization())\n",
    "model_14.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_14.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_14.add(Activation('relu'))\n",
    "model_14.add(BatchNormalization())\n",
    "model_14.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_14.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_14.add(Activation('relu'))\n",
    "model_14.add(BatchNormalization())\n",
    "model_14.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_14.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_14.add(Activation('relu'))\n",
    "model_14.add(BatchNormalization())\n",
    "model_14.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_14.add(Flatten())\n",
    "\n",
    "model_14.add(Dense(128, activation='relu'))\n",
    "model_14.add(Dropout(0.25))\n",
    "\n",
    "model_14.add(Dense(128, activation='relu'))\n",
    "model_14.add(Dropout(0.25))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_14.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_14.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_14.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train and validate the model \n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "17/17 [==============================] - 61s 4s/step - loss: 2.5481 - categorical_accuracy: 0.2989 - val_loss: 1.4292 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0716_53_40.973445/model-00001-2.57966-0.29563-1.42916-0.46000.h5\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - 25s 1s/step - loss: 1.3561 - categorical_accuracy: 0.4118 - val_loss: 1.2557 - val_categorical_accuracy: 0.3667\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0716_53_40.973445/model-00002-1.35608-0.41176-1.25566-0.36667.h5\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - 28s 2s/step - loss: 1.2831 - categorical_accuracy: 0.4576 - val_loss: 1.1416 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0716_53_40.973445/model-00003-1.29840-0.44959-1.14163-0.50000.h5\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - 25s 1s/step - loss: 1.1859 - categorical_accuracy: 0.5170 - val_loss: 1.1010 - val_categorical_accuracy: 0.5667\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0716_53_40.973445/model-00004-1.18593-0.51703-1.10105-0.56667.h5\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.2740 - categorical_accuracy: 0.4852 - val_loss: 1.0614 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0716_53_40.973445/model-00005-1.27561-0.48562-1.06145-0.60000.h5\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 1.2722 - categorical_accuracy: 0.4567 - val_loss: 1.1157 - val_categorical_accuracy: 0.6333\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0716_53_40.973445/model-00006-1.27217-0.45675-1.11572-0.63333.h5\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.1819 - categorical_accuracy: 0.5190 - val_loss: 1.1952 - val_categorical_accuracy: 0.4667\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0716_53_40.973445/model-00007-1.18191-0.51903-1.19523-0.46667.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 1.0432 - categorical_accuracy: 0.5848 - val_loss: 0.8460 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0716_53_40.973445/model-00008-1.04324-0.58478-0.84605-0.65000.h5\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.9724 - categorical_accuracy: 0.6021 - val_loss: 0.9011 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0716_53_40.973445/model-00009-0.97239-0.60208-0.90112-0.68333.h5\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.8241 - categorical_accuracy: 0.6644 - val_loss: 1.0761 - val_categorical_accuracy: 0.5667\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0716_53_40.973445/model-00010-0.82408-0.66436-1.07614-0.56667.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.8663 - categorical_accuracy: 0.6367 - val_loss: 0.8152 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0716_53_40.973445/model-00011-0.86633-0.63668-0.81522-0.65000.h5\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.8021 - categorical_accuracy: 0.6817 - val_loss: 0.7254 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0716_53_40.973445/model-00012-0.80214-0.68166-0.72539-0.70000.h5\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.6939 - categorical_accuracy: 0.7336 - val_loss: 0.8607 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0716_53_40.973445/model-00013-0.69387-0.73356-0.86067-0.60000.h5\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.6444 - categorical_accuracy: 0.7474 - val_loss: 0.9818 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0716_53_40.973445/model-00014-0.64443-0.74740-0.98181-0.58333.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.5822 - categorical_accuracy: 0.7509 - val_loss: 0.6842 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0716_53_40.973445/model-00015-0.58223-0.75087-0.68415-0.78333.h5\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.5564 - categorical_accuracy: 0.7751 - val_loss: 0.8103 - val_categorical_accuracy: 0.6333\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0716_53_40.973445/model-00016-0.55639-0.77509-0.81032-0.63333.h5\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.5119 - categorical_accuracy: 0.7993 - val_loss: 0.8457 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0716_53_40.973445/model-00017-0.51195-0.79931-0.84573-0.66667.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.5886 - categorical_accuracy: 0.7439 - val_loss: 0.5562 - val_categorical_accuracy: 0.8333\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0716_53_40.973445/model-00018-0.58863-0.74394-0.55624-0.83333.h5\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.5537 - categorical_accuracy: 0.7855 - val_loss: 0.8604 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0716_53_40.973445/model-00019-0.55371-0.78547-0.86039-0.65000.h5\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.4925 - categorical_accuracy: 0.7958 - val_loss: 0.7859 - val_categorical_accuracy: 0.6333\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0716_53_40.973445/model-00020-0.49254-0.79585-0.78593-0.63333.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - 21s 1s/step - loss: 0.4670 - categorical_accuracy: 0.8062 - val_loss: 0.6776 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-02-0716_53_40.973445/model-00021-0.46696-0.80623-0.67757-0.75000.h5\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.3786 - categorical_accuracy: 0.8581 - val_loss: 0.7216 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-02-0716_53_40.973445/model-00022-0.37861-0.85813-0.72162-0.76667.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.4176 - categorical_accuracy: 0.8408 - val_loss: 0.7406 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-02-0716_53_40.973445/model-00023-0.41758-0.84083-0.74059-0.70000.h5\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - 21s 1s/step - loss: 0.4275 - categorical_accuracy: 0.8478 - val_loss: 0.7653 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-02-0716_53_40.973445/model-00024-0.42751-0.84775-0.76530-0.75000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.3842 - categorical_accuracy: 0.8581 - val_loss: 0.6925 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-02-0716_53_40.973445/model-00025-0.38416-0.85813-0.69254-0.71667.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff3f5366160>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us fit the model\n",
    "model_14.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "\n",
    "- ***Best Training Accuracy - 80.00 % approx***\n",
    "- ***Best Validation Accuracy - 75.00 % approx***\n",
    "\n",
    "Computational Incapabilities can be handled and the model is a good fit.\n",
    "\n",
    "**Best Model to be used incase there are memory constraints as it can achieve the same accuracy at a lower number of parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run Model 3 to check if the overall accuracy does not drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extra piece of code that was changed to check the model after restarting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_6 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 30, 120, 120, 16)  3472      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 15, 60, 60, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 15, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 15, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 7, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 7, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 7, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 3, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 3, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 3, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              6273000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let us experiment different x,y,z value in the CNN network and find tune all the image size & Hyperparameters later\n",
    "\n",
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "# Input all the images sequencial by building the layer with dropouts and batchnormalisation\n",
    "\n",
    "model_Three = Sequential()       \n",
    "model_Three.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(x,y,z,3),padding='same'))\n",
    "model_Three.add(BatchNormalization())\n",
    "model_Three.add(Activation('relu'))\n",
    "\n",
    "model_Three.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "model_Three.add(Activation('relu'))\n",
    "model_Three.add(BatchNormalization())\n",
    "model_Three.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_Three.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_Three.add(Activation('relu'))\n",
    "model_Three.add(BatchNormalization())\n",
    "model_Three.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_Three.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_Three.add(Activation('relu'))\n",
    "model_Three.add(BatchNormalization())\n",
    "model_Three.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_Three.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_Three.add(Activation('relu'))\n",
    "model_Three.add(BatchNormalization())\n",
    "model_Three.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_Three.add(Flatten())\n",
    "\n",
    "model_Three.add(Dense(1000, activation='relu'))\n",
    "model_Three.add(Dropout(0.5))\n",
    "\n",
    "model_Three.add(Dense(500, activation='relu'))\n",
    "model_Three.add(Dropout(0.55))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_Three.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_Three.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_Three.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train and validate the model \n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "17/17 [==============================] - 97s 6s/step - loss: 8.6027 - categorical_accuracy: 0.2964 - val_loss: 7.4226 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-02-0717_41_34.923192/model-00001-8.64316-0.29412-7.42261-0.41000.h5\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - 41s 2s/step - loss: 6.9656 - categorical_accuracy: 0.3734 - val_loss: 10.1191 - val_categorical_accuracy: 0.3167\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-02-0717_41_34.923192/model-00002-6.96559-0.37340-10.11912-0.31667.h5\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - 45s 3s/step - loss: 5.7980 - categorical_accuracy: 0.4248 - val_loss: 4.7076 - val_categorical_accuracy: 0.4667\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-02-0717_41_34.923192/model-00003-5.88857-0.42234-4.70760-0.46667.h5\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - 38s 2s/step - loss: 3.4030 - categorical_accuracy: 0.4923 - val_loss: 2.4368 - val_categorical_accuracy: 0.5333\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-02-0717_41_34.923192/model-00004-3.40301-0.49226-2.43684-0.53333.h5\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - 39s 2s/step - loss: 2.4193 - categorical_accuracy: 0.5369 - val_loss: 6.4140 - val_categorical_accuracy: 0.3167\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-02-0717_41_34.923192/model-00005-2.44029-0.53355-6.41404-0.31667.h5\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 2.0713 - categorical_accuracy: 0.5536 - val_loss: 2.1248 - val_categorical_accuracy: 0.5333\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-02-0717_41_34.923192/model-00006-2.07127-0.55363-2.12476-0.53333.h5\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 1.8188 - categorical_accuracy: 0.5571 - val_loss: 1.6193 - val_categorical_accuracy: 0.4167\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-02-0717_41_34.923192/model-00007-1.81883-0.55709-1.61929-0.41667.h5\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 1.3907 - categorical_accuracy: 0.5744 - val_loss: 2.6272 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-02-0717_41_34.923192/model-00008-1.39070-0.57439-2.62719-0.45000.h5\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 1.6615 - categorical_accuracy: 0.5260 - val_loss: 1.2433 - val_categorical_accuracy: 0.6333\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-02-0717_41_34.923192/model-00009-1.66146-0.52595-1.24329-0.63333.h5\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.8931 - categorical_accuracy: 0.7232 - val_loss: 0.8790 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-02-0717_41_34.923192/model-00010-0.89312-0.72318-0.87897-0.65000.h5\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 1.1347 - categorical_accuracy: 0.6782 - val_loss: 0.6660 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-02-0717_41_34.923192/model-00011-1.13474-0.67820-0.66597-0.70000.h5\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 0.8554 - categorical_accuracy: 0.7336 - val_loss: 0.8766 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-02-0717_41_34.923192/model-00012-0.85542-0.73356-0.87661-0.66667.h5\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.7588 - categorical_accuracy: 0.7543 - val_loss: 0.7975 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-02-0717_41_34.923192/model-00013-0.75876-0.75433-0.79752-0.73333.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 0.7497 - categorical_accuracy: 0.7301 - val_loss: 0.6890 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-02-0717_41_34.923192/model-00014-0.74972-0.73010-0.68905-0.68333.h5\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.5538 - categorical_accuracy: 0.7855 - val_loss: 0.4507 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-02-0717_41_34.923192/model-00015-0.55380-0.78547-0.45070-0.80000.h5\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 0.4665 - categorical_accuracy: 0.8512 - val_loss: 0.6723 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-02-0717_41_34.923192/model-00016-0.46651-0.85121-0.67232-0.80000.h5\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.4168 - categorical_accuracy: 0.8547 - val_loss: 0.5207 - val_categorical_accuracy: 0.8167\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-02-0717_41_34.923192/model-00017-0.41676-0.85467-0.52067-0.81667.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.3165 - categorical_accuracy: 0.8893 - val_loss: 0.6614 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-02-0717_41_34.923192/model-00018-0.31651-0.88927-0.66144-0.75000.h5\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.3562 - categorical_accuracy: 0.8789 - val_loss: 0.4867 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-02-0717_41_34.923192/model-00019-0.35622-0.87889-0.48673-0.85000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 0.2934 - categorical_accuracy: 0.8893 - val_loss: 0.3567 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-02-0717_41_34.923192/model-00020-0.29344-0.88927-0.35674-0.90000.h5\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - 36s 2s/step - loss: 0.2428 - categorical_accuracy: 0.8997 - val_loss: 0.6587 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-02-0717_41_34.923192/model-00021-0.24281-0.89965-0.65872-0.75000.h5\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.2776 - categorical_accuracy: 0.8962 - val_loss: 0.3672 - val_categorical_accuracy: 0.8667\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-02-0717_41_34.923192/model-00022-0.27765-0.89619-0.36717-0.86667.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.2177 - categorical_accuracy: 0.9066 - val_loss: 0.6270 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-02-0717_41_34.923192/model-00023-0.21775-0.90657-0.62704-0.75000.h5\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.1887 - categorical_accuracy: 0.9343 - val_loss: 0.3852 - val_categorical_accuracy: 0.8833\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-02-0717_41_34.923192/model-00024-0.18869-0.93426-0.38524-0.88333.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.2739 - categorical_accuracy: 0.9100 - val_loss: 0.6248 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-02-0717_41_34.923192/model-00025-0.27388-0.91003-0.62475-0.78333.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7709fe4ac8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let us fit the model\n",
    "model_Three.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "\n",
    "- ***Best Training Accuracy - 85.50 %***\n",
    "- ***Best Validation Accuracy - 82.00 %***\n",
    "\n",
    "We can clearly see that increasing epoch have increased accuracy.\n",
    "\n",
    "The above are best values we got in model-3. Going with epoch-25's values as the difference between training and validation accuracy is <5%.\n",
    "\n",
    "The computation time increases with the number of epochs, however the accuracy also increases and gradually the model runs better. \n",
    "\n",
    "Since, we are more emphasizing on performance now and not computation time, let’s still use model-3 as our final model. \n",
    "\n",
    "Currently, we have obtained our best h5 model file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
